---
{"dg-publish":true,"permalink":"/1/110-paper-summaried/112-attention-all-you-need/","title":"Attention all you need","tags":["paper","summary","gardenEntry"]}
---

# Attention all you 
## Back ground
자연어 처리에서 연구를 발돋음 시킨 논문이자 현재 많은 논문들이 사용p하는 transfomer 구조를 처음 제시한 논문이다. 긴 문장에 대한 메모리 문제를 해결하려 많은 도전이 있었지만 가장 효과적이고 심플한 구조를 제시하였다. 
두 임의의 입력 또는 출력 위치에서 신호를 연관시키는 데 필요한 연산 횟수가 증가한다. 예를 들어  ConvS2S는 linearly, ByteNet 
## Architecture 

![Transfomer.png](/img/user/0.%EC%A7%80%EC%8B%9D%EC%B0%BD%EA%B3%A0/030.%20source/Transfomer.png)
위의 그림은 논문에서 제시된 구조이다.  Encoder 와 Decoder 구조를 가지고 있으며, 논문에선 각각 6개의 레이어를 가지고 있다고 설명한다.  Encoder의 input 으로는 Q, K, V 가 들어가게 되어 있으며 두개의 서브 레이어를 가지고 있다. Multi-head attention layer  와  FC layer를 가지고 있으며, skip connection 을 이용하여 나온 결과를 합친 후 Norm 을 진행한다. 
Decoder 는 Encoder와 유사한 구조를 가지고 있다.  3개의 sub layer 중  encoder에서 나온 값을 사용하는 sublayer 가 추가된 형태이다. 
### Attention module
![self-attention.png](/img/user/0.%EC%A7%80%EC%8B%9D%EC%B0%BD%EA%B3%A0/030.%20source/self-attention.png)
식으로 표현하자면 $$Attention(Q,K,V) = (\frac{QK^T}{\sqrt{d_k}})V$$ 와 같다 . Q, K, V는  input에서 나눠진 값들이며,  K 를 transform을 진행하는 이유는 
$$ Q = K = V = \begin{bmatrix}
    x_{11} & x_{12} & x_{13} & \dots  & x_{1n} \\
    x_{21} & x_{22} & x_{23} & \dots  & x_{2n} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    x_{d1} & x_{d2} & x_{d3} & \dots  & x_{dn}
\end{bmatrix}$$ 

이기 때문에 행렬곱 을 진행하기 위해 Transform을 진행한다.  

### Multi head attention 
![[multihead.png \|multihead.png ]]
Q,K,V 를  각각을  $d_k, d_k, d_v$ 차원으로  Linear Projection을 진행하여 성능 향상을 꾀한다
$$MultiHead(Q,K,v) = Concat(head_1,...,head_i)$$
$$where head_i = Attention(QW^Q_i, KW^K_i, VW^V_i)$$
단일 Attention에서 정보 손실의 위험이 있으며, 문장이 길어짐에 따라 계산 속도가 느려지는 문제 그리고 결정적으로 문장에 대한 전체 이해도가 아닌 단일 워드에 대한 집중이 다보니 모델의 신뢰도가 낮은편이다.
Multi head attention은 병렬로 다른 정보를 처리함으로써 더 많은 정보를 계산할 수 있게 하였으며, 각 head 별로 sequnce의 다른 부분을 학습하기 때문에 sequence 파악 능력이 향상되진다

### Position-wise Feed-Forward Netwroks
Fully connected feed foward와 Relu를 사용하고 있으며 레이별 다른 parameter를 사용하고 있다 $FFN(x) = max(0, xW_1 + b_1)W_2 + b_2$ 로  이는 $ReLU(x) = max(0,x)$식을 활용하여 선형 계산을 진행한 결과이다

### Embeddings and Softmax
학습된 임베딩을 활용하여 input token 으로 활용하며 Softmax를 사용하여 디코더 출력을 확율 token으로 변형 한다
### Position Encoding
해당 논문은 기존 모델들과 다르기 때문에 각 토큰의 위치 정보를 전달해 줄 필요가 있다. 
$PE_{(pos,2i)} = sin(pos / 10000^{2i/d_model}), PE_{(pos,2i+1)} = cos(pos / 10000^{2i/d_model})$ 로 계산 되어진다. pos 는 위치를 뜻하고 i는 차원을 의미한다.


## 생각
지금 와서는 너무나도 당연하게 쓰이고 있는 개념이라 생각한다. 예전에도 해당 논문을 구현 및 리뷰를 했었지만 시간도 오래 지났고 계속해서 vision 모델, 최근들어 CNN을 주로 사용하는 도메인을 활용하면서 까먹었던 부분을 상기하기 위해 다시 한번 읽어 보았다
Mutli head 부분은 병렬 처리를 통해 모델의 성능 자체를 끌어 올린 좋은 아이디어이다. 난 자연어 도메인을 잘 모르기때문에 위치 인코딩에서 많은 설명이 부족하다. 이 부분은 공부를 통해 채워 가야할 부분으로 보인다

