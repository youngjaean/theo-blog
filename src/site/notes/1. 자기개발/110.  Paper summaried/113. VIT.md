---
{"dg-publish":true,"permalink":"/1/110-paper-summaried/113-vit/","title":"VIT","tags":["paper","summary","study"]}
---

# VIT
[[1. 자기개발/110.  Paper summaried/112. Attention all you need\|Base]]
## Back ground

- 이전까지 NLP에서만 사용되던 Transformer  architecture를 이미지에 적용한 논문으로, 이 후 대부분의 논은 multi head Self-attention으로 연구가 진행되었다. 
- 최근 들어 Object detection, Segmentation 까지 많은 영역에서 사용되고 있는 구조인만큼 [[1. 자기개발/110.  Paper summaried/112. Attention all you need\|112. Attention all you need]] 와 함께 이해도를 높이면 좋은 논문이다
## Architecture
![Pasted image 20240512163422.png](/img/user/0.%EC%A7%80%EC%8B%9D%EC%B0%BD%EA%B3%A0/030.%20source/Pasted%20image%2020240512163422.png)
- 위와 같은 구조를 가진 모델로, Bert의 구조를 차용하였음
- 중요한건 이미지를 flatten 시켜 sequential한 구조를 차용하였으며, 이미지의 위치 특성을 유지하기 위해 Position Embedding을 활용하다.
- Encoder 부분을 활용하였으며, Class embedding 과 Position Embedding은 leranable parameter 주어진다
- MLP Head를 통해 이미지를 예측 할 수있다 
	-  'Head를 바꾼다면 다른걸할 수 있었을까?' 에 대한 답변을 Bert를 찾아보고 이해한뒤 정리해보자

## Detail
- Image is $x \in \mathbb{R}^{H\times W\times C}$ 를 2D flattene를 위해 patch $x_p \in \mathbb{R}^{N\times ( P^2 \cdot C)}$ 로 나뉘어 짐
- H, W 는 원본 이미지와 동일하며, C 는 input channel 이며 (P, P)는각 이미지 패치의 resolution , $N = HW/P^2$ 
- The Transformer uses constant latent vector size D through all of its layers, so we flatten the patches and map to D dimensions with a trainable linear projection(?)
	- $z_0 = [x_{class}; x_p^1\textbf{E};x_p^22\textbf{E};...;x_p^n\textbf{E};] + \textbf{E}_{pos}, \qquad \textbf{E} \in \mathbb{R}^{(P^2 \cdot C) \times D } , \textbf{E}_{pos} \in \mathbb{R}^{(N+1)\times D}$   에서 $\textbf{E}_{pos} \in \mathbb{R}^{(N+1)\times D}$ 은 position encoding을 표현
	- $x_{class}$ 는 학습 가능한 classification token
- MSA(Multi Self-attention), MLP(Multilayer perceptron) 를 각각  통과
	- $z^{`}_{\ell} = MSA(LN(\textbf{z}_{\ell -1})) + \textbf{z}_{\ell - 1}$ ,  $z_{\ell} = MLP(LN(\textbf{z}^{`}_{\ell})) + \textbf{z}^{`}_{\ell}$ 
- 마지막으로 Layer norm 을통한 결과값 예측
	- $y = LN(z^0_L)$
## 성능
![[Pasted image 20240512191520.png \| 500]]

- Pre-train에 사용되는 데이터는 매우 많은 양이 들어가야함
	- ![Pasted image 20240512191640.png](/img/user/0.%EC%A7%80%EC%8B%9D%EC%B0%BD%EA%B3%A0/030.%20source/Pasted%20image%2020240512191640.png)
- 위 그래프에서 볼 수 있듯이, 데이터양이 부족하다면 Convolution 보다 성능이 낮음

## 참고할 만한 연구
 ![Pasted image 20240512191834.png](/img/user/0.%EC%A7%80%EC%8B%9D%EC%B0%BD%EA%B3%A0/030.%20source/Pasted%20image%2020240512191834.png)
 - 학습이 충분하게 됫을경우 Embedding Filter가 CNN과 유사한 역할을 하고 있음
 - 두번째 그림에서 보듯이 position embedding 이 이미지의 위치를 학습하는것을 보여줌
 - <font color="#245bdb">가장 중요한건 해당 방법론을 시각화 하는 방법을 찾아봐야함</font>
 - 