---
{"dg-publish":true,"permalink":"/1-paper/130-theory/132-sharpness-aware-minimization-sam/","title":" Sharpness-Aware Minimization (SAM)"}
---

# Sharpness-Aware Minimization (SAM)

## 1. SAM이란?

SAM은 모델의 일반화 성능을 효율적으로 향상시키기 위한 최적화 기법

### As-Is (기존 방식)
- 최근 모델들은 과도하게 매개변수화(overparameterized)되는 경향
- 기존의 최적화 방법(SGD, Adam 등)은 종종 부분최적해(suboptimal)에 빠지짐

### To-Be (SAM의 접근)
- 이를 방지하기 위해 주변과 균일하게 낮은 손실값을 찾는 방식을 채택

## 2. SAM의 방법론

### 2.1 주요 아이디어
- 테스트 손실값을 다음과 같이 표현:
  $L_\mathcal{D}(w) \leq L_s(w + \epsilon) + h\|w\|_2^2 + \frac{p}{2}$

- 이를 다음 식으로 변환하여 sharpness term을 최소화:
  $L_\mathcal{D}(w) \leq \max_{\|\epsilon\|_2 \leq \rho} [L_s(w + \epsilon) - L_s(w)] + L_s(w) + h\|w\|_2^2 + \frac{p}{2}$

### 2.2 주요 특징
1. $w$에 근처 값을 더했을 때의 변화량을 고려
2. 이를 통해 sharpness term 계산이 가능
3. $L_s(w)$ 항은 0으로 정규화

### 2.3 계산 과정
- 엡실론 변화는 1차 Taylor 전개와 Dual norm problem을 통해 계산 가능한 식으로 변환
- 최종적으로 다음과 같은 식이 도출
  $\nabla_w L_s^{SAM}(w) \approx \nabla_w L_s(w)|_{w+\hat{\epsilon}(w)}$

## 3. 결론

SAM은 손실 함수의 국소적 기하학을 고려하여 보다 일반화 가능한 해를 찾는 방법을 제시합니다. 이는 과적합을 줄이고 모델의 성능을 향상시키는 데 도움