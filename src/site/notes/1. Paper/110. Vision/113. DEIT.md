---
{"dg-publish":true,"permalink":"/1-paper/110-vision/113-deit/","title":" 113. DEIT","tags":["paper","backbone","summary"]}
---

# Training data-efficient image transformers & distillation through attention
## 2024-07-08
## Background

- 공개된 데이터셋, 8-GPU, 53시간 내외의 학습을 통해 ViT보다 높은 성능을 보인다.
- Vision Transformer는 구글의 비공개 대용량 데이터로 학습시켜, 일반 연구원들이 처음부터 학습을 시켜서 성능 검증을 하기에 어려운 점이 존재한다.
- 본 논문에서는 Distillation을 통해 작은 데이터로 높은 성능을 이끌어 냈다.
- Transformers 구조와 Teacher-Student 기법을 활용하여 효율적인 학습 및 성능을 이끌어 냈다고 할 수 있다.
## Model Architecture
![JPEG image-47D0-8F1A-3B-0.jpeg](/img/user/0.%EC%A7%80%EC%8B%9D%EC%B0%BD%EA%B3%A0/030.%20source/JPEG%20image-47D0-8F1A-3B-0.jpeg)
- Transformer는 [[1. Paper/110. Vision/111. VIT\|Vit]]와 동일한 구조를 차용하였다. 저자는 distillation token을 추가하여 학습을 진행하였으며 본 논문의 쟁점은 distillation이다.
- DeiT는 ViT의 기본 구조에 distillation token을 추가했다. 이 token은 class token과 병렬로 처리되며, teacher 모델의 출력을 모방하도록 학습된다.
- Distillation을 통해 상대적으로 작은 데이터로 충분한 성능을 선보였다.
- Distillation token은 class token과 동일하게 학습 가능하며 랜덤으로 초기화된다.

## Distillation
- Teacher-student 모델에서 convnet teacher 모델이 transformer 모델 학습을 위해 class token과 유사한 distillation token을 학습하게 되고 이를 통해 효과적인 distillation을 진행할 수 있다.
- Distillation token은 teacher 모델의 지식을 학생 모델에 전달하는 역할을 하며, attention 메커니즘을 통해 다른 토큰들과 상호작용한다.

### Soft distillation
- Random할수록 entropy가 상승하며, cross entropy loss는 Ground Truth와 Prediction value의 엔트로피 차이를 계산하는 방식이다.
- 이러한 방식이 Kullback-Leibler divergence와 매우 유사하다. Ground Truth와 Prediction value를 근사화하여 entropy를 구하면 된다.
- $\mathcal{L}_{global} = (1- \lambda)\mathcal{L}_{CE}(\psi(Z_s), y) + \lambda\tau^2KL(\psi(Z_s/\tau),\psi(Z_t/\tau))$ 로 표현할 수 있으며, 쉽게 표현하자면 $\mathcal{L}_global = weight_1 \cdot cross entropy + weight_2 \cdot distillation temperature \cdot KL(student predict, teacher predict)$로 표현할 수 있다.
- 여기서 $\psi$는 softmax를 뜻하며 분포를 근사하여 계산하기 용이하게 하기 위해 사용된다.
- 목표는 student 모델과 teacher 모델의 KL 발산을 줄이기 위함이다.

### Hard-label distillation
- $\mathcal{L}^{hardDistill}_{global} = \frac{1}{2} \mathcal{L}_{CE}(\psi(Z_s), y) + \frac{1}{2} \mathcal{L}_{CE}(\psi(Z_s), y_t)$
- Student 모델과 ground truth의 Cross entropy loss와 teacher 모델의 hard predict 값을 통해 loss를 계산한다.

### Distillation token
- Class token과 유사한 distillation token을 활용하여 성능 향상을 노렸으며, 실제 저자는 class token과 다른 값을 가지고 있으며 동일한 목표를 통해 학습하기 때문에 유용하다고 이야기한다.
- Distillation token은 teacher 모델의 출력을 직접적으로 모방하도록 학습되며, 이를 통해 teacher의 지식을 더 효과적으로 학생 모델에 전달할 수 있다.